# This file specifies the post-processing filter pipeline that extract various kinds of information
# from the raw data output by the DSUI instrumentation points when "./simple" was run with all
# instrumentation points implicitly enabled. 
# 
# The "filters" keyword specifies the set of filter pipelines defined by this configuration file.
# In this case, only one pipeline "f1" is defined. The pipeline "f1" begins,as do all pipelines 
# with a somewhat specialized mandatory "head" filter which is used to configure the input for this
# pipeline, and which can handle a variety of situations including merging input from multiple files. 
# In this case the input for piepline "f1" is the raw DSUI binary file "/tmp/simple.dsui.bin". Note 
# also that the "head.input" specification for a pipeline must also include the declaration of all
# entities generated by this pipeline. In this case the SIMPLEPIPE histogram and three FUNC_LOOP
# intervals are the set of entities generated.

<main>
filters = {
	f1 = [
		head.input(
			file = ["/tmp/simple.dsui.bin"]
			namespace = {
				SIMPLEPIPE = {
					desc = "simple pipe"
					entities = {
						TIMEHIST = histogram(
							desc = "the derived histogram"
						        units = "ns"
						)
					}
				}
				FUNC_LOOP = {
					entities = {
						INTERVAL_MEDIAN = interval(
							desc = "median of loop intervals"
						)
						INTERVAL_DERIVED = interval(
							desc = "created from ITER_ONE and ITER_TWO"
						)
						INTERVAL_DERIVED_MEDIAN = interval(
							desc = "median"
						)
					}
				}
			}	
		)

		# F1 - Each entity record contains a raw TSC value.
		# The first entity in a DSUI binary file is a header
		# record whose TSC value represents the beginning of
		# the experiment, and the TSC-to-nanosecond conversion
		# factor for the machine on which the TSC value was
		# generated. This filter converts each entity record's
		# raw TSC value into the ellapsed time since the beginning
		# of the experiment by subtracting the header TSC value
		# and then applying the TSC->ns conversion factor.
		# Nanoseconds are the default unit, but other units can
		# be specified. Note that if more than one raw data file
		# is being merged, determining the TSC for the beginning
		# of the experiment and conversion can be more complex.	
		utility.timestamp()

		# F2 - accumulates the FUNC_LOOP/DSUI_OVERHEAD intervals into
		# the SIMPLEPIPE/TIMEHIST histogram. A snapshot of the accumulated
		# histogram is output when the end of the event stream is
		# detected.
		conversion.interval_to_histogram(
			interval = "FUNC_LOOP/DSUI_OVERHEAD"
			histogram = "SIMPLEPIPE/TIMEHIST"
			buckets = 40
		)

		# F3 - Graphs the SIMPLEPIPE/TIMEHIST histograph
		# snapshot when it passes through.
		graph.histogram(
			histogram = "SIMPLEPIPE/TIMEHIST"
			title = "time hist"
		)

		# F4 - Graphs the FUNC_LOOP/SUM histograph
		# snapshot when it passes by in the entity
		# stream.
		graph.histogram(
			histogram = "FUNC_LOOP/SUM"
			title = "random"
		)

		# F5 - Here we output the records with the
		# converted timestamps into a file for preservation.
		# Note that the output is "pickled" because within
		# the python of the filter, the entity record
		# structure must be serialized for binary output and 
		# subsequent reconstruction of the entity structure
		# when input into some other processing pipeline.
		output.pickle(
			filename = "simple.pp2"
		)

		# F6 - This filter consumes all of the FUNC_LOOP/
		# DSUI_OVERHEAD interval entities and calculates
		# their median value. The median is represented by 
		# a new interval entity FUNC_LOOP/INTERVAL_MEDIAN
		# emitted when this filter detects the end of the 
		# input stream.
		reduction.interval(
			operation = median
			src_interval = "FUNC_LOOP/DSUI_OVERHEAD"
			dest_event = "FUNC_LOOP/INTERVAL_MEDIAN"
			consume = true
		)

		# F7 - This filter converts pairs of simple events
		# into a synthesized interval event. ITER_ONE and 
		# ITER_TWO define the beginning and end of the execution
		# interval in simple.c that we wish to measure. This
		# filter takes the difference between the two and emits
		# INTERVAL_DERIVED whose beginning and end times match the
		# times of ITER_ONE and ITER_TWO respectively. Note that
		# ITER_ONE and ITER_TWO are consumed by this filter.
		conversion.event_to_interval(
			start_event = "FUNC_LOOP/ITER_ONE"
			end_event = "FUNC_LOOP/ITER_TWO"
			interval = "FUNC_LOOP/INTERVAL_DERIVED"
			consume = true
		)

		# F8 - This filter, similar to F6, consumes the synthesized
		# INTERVAL_DERIVED entities, and calculates their median value.
		reduction.interval(
			operation = median
			src_interval = "FUNC_LOOP/INTERVAL_DERIVED"
			dest_event = "FUNC_LOOP/INTERVAL_DERIVED_MEDIAN"
			consume = true
		)

		# F9 - This instance of the narrate filter will output the 
		# entity records remaining the event stream in human readable
		# form. The "divisor" parameter specifies a factor used to devide
		# the nanosecond timestamp values to make more easily readable.
		# This factor of one million obviously converts nanoseconds to
		# milliseconds. Reading simple.narrate.txt is an excellent
		# excercise. It begins with a series of administrative events
		# not inthe application and includes all the simple.c events
		# not explicitly consumed by earlier pipeline stages as well as
		# some of the "generated" events.
		utility.narrate(
			divisor = 1000000
			output = "simple.narrate.txt"
		)
	]
}





